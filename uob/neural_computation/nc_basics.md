---
layout: page
title: Basics
---
## [Activation functions](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)
![](images/activations.png)
![](https://miro.medium.com/max/1400/1*n1HFBpwv21FCAzGjmWt1sg.png)
|                            | output           |                |                                                                     |                                                                                          |
|----------------------------|------------------|----------------|---------------------------------------------------------------------|------------------------------------------------------------------------------------------|
| linear/I                   | +-infinity       | differentiable | ![](https://miro.medium.com/max/970/1*Xu7B5y9gp0iL5ooBj7LtWw.png)   | cause a neural network to get stuck at the training time.                                |
| Sigmoid/Logistic           | (0 to 1)         | differentiable | ![](https://miro.medium.com/max/1190/1*f9erByySVjTjohfFdNkJYQ.jpeg) |                                                                                          |
| Tanh/hyperbolic tangent    | (-1 to 1)        | differentiable | ![](https://miro.medium.com/max/1190/1*f9erByySVjTjohfFdNkJYQ.jpeg) | mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. |
| ReLU/Rectified Linear Unit | [ 0 to infinity) |                | ![](https://miro.medium.com/max/1400/1*A_Bzn0CjUgOXtPCJKnKLqA.jpeg) | not mapping the negative values appropriately.                                           |
| Leaky ReLU                 | +-infinity       |                |                                                                     | attempt to solve the dying ReLU problem                                                  |
# Linear Regression and Gradient Descent


# MLP
# BP
# OPT

# CNN
# RNN
